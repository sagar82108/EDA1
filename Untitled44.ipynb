{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98abf4d-4075-44d1-8135-1e979eccdb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fixed Acidity\n",
    "Volatile Acidity\n",
    "Citric Acid\n",
    "Residual Sugar\n",
    "Chlorides\n",
    "Free Sulfur Dioxide\n",
    "Total Sulfur Dioxide\n",
    "Density\n",
    "pH\n",
    "Sulphates\n",
    "Alcohol\n",
    "Quality (Target variable)\n",
    "The \"wine quality\" dataset contains features related to the chemical composition of wines, \n",
    "including acidity, sugar, sulfur dioxide levels, and more.\n",
    "These features are important in predicting wine quality, as they impact taste, aroma, and overall appeal to consumers.\n",
    "The target variable is \"Quality,\" a subjective rating given by experts. Predictive models use these features to estimate wine quality based on observed relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88228536-f21a-4a8c-a760-c4976e116743",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling missing data in the wine quality dataset can be done using techniques like removal of missing data (simple but can lead to data loss), \n",
    "mean/median imputation (simple but can introduce bias), regression imputation (considers variable relationships), mode imputation for categorical data,\n",
    "and more. Multiple imputation is a robust choice, generating multiple imputed datasets. The best technique depends on data characteristics and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9beaf-6dc3-4dde-b8ea-50eb07840795",
   "metadata": {},
   "outputs": [],
   "source": [
    "Factors affecting students' exam performance include study habits, prior knowledge, motivation, teacher quality, and more. To analyze these factors statistically:\n",
    "\n",
    "1. Collect relevant data.\n",
    "2. Clean and preprocess the data.\n",
    "3. Use descriptive statistics, correlation analysis, regression, hypothesis testing, and data visualization.\n",
    "4. Consider advanced techniques like machine learning for deeper insights.\n",
    "5. Interpret findings to improve student performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35032d-53c1-4fc1-b21e-126a40db2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the student performance dataset, feature engineering involves selecting, transforming, and creating variables to improve predictive models. \n",
    "Steps include data exploration, feature selection, handling categorical variables, scaling, creating new features, handling missing data, iteration,\n",
    "dimensionality reduction (if needed), and validation. The goal is to enhance model performance and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136dd5ba-89d5-43fc-be66-7171fab45eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform EDA on the wine quality dataset:\n",
    "\n",
    "1. Load the data.\n",
    "2. Calculate summary statistics.\n",
    "3. Create histograms, probability plots, and density plots.\n",
    "4. Assess skewness and kurtosis.\n",
    "5. Use visualizations to identify outliers.\n",
    "6. Apply statistical tests for normality.\n",
    "7. If non-normality is found, consider transformations like logarithmic, square root, Box-Cox, or inverse to improve normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742c2e8-2eab-4d36-ad44-b5190ef9ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the wine quality dataset\n",
    "data = pd.read_csv(\"wine_quality.csv\")\n",
    "\n",
    "# Separate features and target variable (assuming 'Quality' is the target)\n",
    "X = data.drop(columns=[\"Quality\"])\n",
    "y = data[\"Quality\"]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate explained variance and cumulative explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variance.cumsum()\n",
    "\n",
    "# Find the minimum number of components for 90% explained variance\n",
    "min_components = sum(cumulative_explained_variance >= 0.9)\n",
    "\n",
    "print(f\"Minimum number of components for 90% explained variance: {min_components}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
